# PSBench
A comprehensive benchmark for estimating the accuracy of protein complex structural models (EMA)
![PSBench Pipeline, Methods and Metrics](Datasets/imgs/pipeline_methods_metrics.png)

PSBench datasets are publicly available at https://dataverse.harvard.edu/previewurl.xhtml?token=bd3a9914-24c6-4acb-a6c1-6886dc52aa4b
(need to change after publishing)

DOI : https://doi.org/10.7910/DVN/75SZ1U
(might change after publishing)
## PSBench Installation

### Clone the repository
```
git clone https://github.com/BioinfoMachineLearning/PSBench.git
cd PSBench/ 
```
### Setup the environment
```
#create the PSBench environment
conda create -n PSBench python=3.10.12

# activate PSBench
conda activate PSBench

#install the required packages
pip install -r scripts/requirements.txt
```
### Setup and test OpenStructure
```
# install OpenStructure
docker pull registry.scicore.unibas.ch/schwede/openstructure:latest

# test OpenStructure installation 
docker run -it registry.scicore.unibas.ch/schwede/openstructure:latest --version
```

## I. Datasets for training and testing EMA methods
PSBench consists of 4 complementary large datasets and two additional subsets of inhouse datasets which were used by GATE:
1. CASP15_inhouse_dataset
2. CASP15_inhouse_TOP5_dataset (subset of CASP15_inhouse_dataset)
3. CASP15_community_dataset
4. CASP16_inhouse_dataset
5. CASP16_inhouse_TOP5_dataset (subset of CASP16_inhouse_dataset)
6. CASP16_community_dataset


## The dataset directory structure

```text
ğŸ“ PSBench/
â”œâ”€â”€ ğŸ“ CASP15_inhouse_dataset/
â”‚   â”œâ”€â”€ ğŸ“„ CASP15_inhouse_dataset_summary.tab
â”‚   â”œâ”€â”€ ğŸ“ AlphaFold_Features/
â”‚   â”œâ”€â”€ ğŸ“ Fasta/
â”‚   â”œâ”€â”€ ğŸ“ Predicted_Models/
â”‚   â””â”€â”€ ğŸ“ Quality_Scores/
â”œâ”€â”€ ğŸ“ CASP15_inhouse_TOP5_dataset/
â”‚   â”œâ”€â”€ ğŸ“„ CASP15_inhouse_TOP5_dataset_summary.tab
â”‚   â”œâ”€â”€ ğŸ“ AlphaFold_Features/
â”‚   â”œâ”€â”€ ğŸ“ Fasta/
â”‚   â””â”€â”€ ğŸ“ Quality_Scores/
â”œâ”€â”€ ğŸ“ CASP15_community_dataset/
â”‚   â”œâ”€â”€ ğŸ“„ CASP15_community_dataset_summary.tab
â”‚   â”œâ”€â”€ ğŸ“ Fasta/
â”‚   â”œâ”€â”€ ğŸ“ Predicted_Models/
â”‚   â””â”€â”€ ğŸ“ Quality_Scores/
â”œâ”€â”€ ğŸ“ CASP16_inhouse_dataset/
â”œâ”€â”€ ğŸ“ CASP16_inhouse_TOP5_dataset/
â”œâ”€â”€ ğŸ“ CASP16_community_dataset/
â”œâ”€â”€ ğŸ“„ extract.sh
â””â”€â”€ ğŸ“„ README.md

```

Note: The TOP5 subsets (CASP15_inhouse_TOP5_dataset and CASP16_inhouse_TOP5_dataset) do not include the Predicted_Models directories to minimize redundancy and optimize storage. These models are identical to those already available in their respective full datasets (CASP15_inhouse_dataset/Predicted_Models/ and CASP16_inhouse_dataset/Predicted_Models/).


For each of the datasets, we provide 10 unique quality scores and a few AlphaFold features:

| Category | Quality scores / features |
|:---------|:-------------------|
| **Global Quality Scores** | tmscore (4 variants), rmsd |
| **Local Quality Scores** | lddt |
| **Interface Quality Scores** | ics, ics_precision, ics_recall, ips, qs_global, qs_best, dockq_wave |
| **Additional Input Features** (CASP15_inhouse_dataset and CASP16_inhouse_dataset) | type, afm_confidence_score, af3_ranking_score, iptm, num_inter_pae, mpDockQ/pDockQ |

For detailed explanations of each quality score and feature, please refer to [Quality_Scores_Definitions](Datasets/Quality_Scores_Definitions.json)

<details>
  
For each figures below, **(a) Model count.** Number of models per target in the dataset. **(b) Score Distribution.** Box plots of each of six representative quality scores of the models for each target. **(c) Example.** Three representative models (worst, average, best) in terms of sum of the six representative quality scores for a target. Each model with two chains colored in blue and red is superimposed with the true structure in gray.

## i. CASP15_inhouse_dataset
CASP15_inhouse_dataset consists of a total of 7,885 models generated by MULTICOM3 during the 2022 CASP15 competition. Example target in Figure (c): H1143. 
![CASP15_inhouse_dataset](Datasets/imgs/CASP15_inhouse_dataset.png)

**CASP15_inhouse_TOP5_dataset** is a subset curated for GATE-AFM from the **CASP15_inhouse_dataset**, consisting of the top 5 models per predictor. Each predictor varies in its use of input multiple sequence alignments (MSAs), structural templates, and AlphaFold configuration parameters to generate structural models using the AlphaFold. 

## ii. CASP15_community_dataset
CASP15_community_dataset consists of a total of 10,942 models generated by all the participating groups during the 2022 CASP15 competition. Example target in Figure (c): H1135. 
![CASP15_community_dataset](Datasets/imgs/CASP15_community_dataset.png)

## iii. CASP16_inhouse_dataset
CASP16_inhouse_dataset consists of a total of 1,009,050 models generated by MULTICOM4 during the 2024 CASP16 competition. Example target in Figure (c): T1235o. 
![CASP16_inhouse_dataset](Datasets/imgs/CASP16_inhouse_dataset.png)

**CASP16_inhouse_TOP5_dataset** is a subset curated for GATE-AFM from the **CASP16_inhouse_dataset**, consisting of the top 5 models per predictor. Each predictor varies in its use of input multiple sequence alignments (MSAs), structural templates, and AlphaFold configuration parameters to generate structural models using the AlphaFold program. 

## iv. CASP16_community_dataset
CASP16_community_dataset consists of a total of 12,904 models generated by all the participating groups during the 2024 CASP16 competition. Example target in Figure (c): H1244. 
![CASP16_community_dataset](Datasets/imgs/CASP16_community_dataset.png)
</details>

## II. Scripts to evaluate EMA methods on a benchmark dataset

This script evaluates Pearson correlation, Spearman correlation, top-1 loss, and ROC AUC for one or more scoring fields.

### Example command:

```bash
cd scripts
python evaluate_QA.py \
  --indir ./predictions \
  --nativedir ./native_scores \
  --native_score_field tmscore_usalign
```

### Arguments:

| Argument               | Description |
|------------------------|-------------|
| `--indir`              | Directory with prediction CSV files |
| `--nativedir`          | Directory with native score CSV files |
| `--native_score_field` | Column name of native score. Default is `tmscore_usalign` |
| `--field`              | (Optional) Score column to evaluate. If omitted, all columns from column 2 onward are evaluated, assuming 'model' is column 1 |

### Prediction files (CSV format):
Each file should contain:
```
model,EMA_score1,EMA_score2,...
model1,0.85,0.79
model2,0.67,0.71
```

### Output:

The script prints:
- The evaluated fields (EMA score names)
- Target names (e.g., H1106)
- For each target:
  - Pearson correlation
  - Spearman correlation
  - Top-1 loss: native score difference between the best model and the top-1 model ranked by the EMA scores
  - RUAOC using top 25% native score threshold
    
## III. Scripts to generate labels for a new benchmark dataset
Following are the prerequisites to generate the labels for new benchmark dataset:
### Data:
- Predicted structures
- Native structures
- Fasta files
### Tools (Downloaded or installed in the PSBench installation section)
 - OpenStructure
 - USalign
 - Clustalw

<!-- <details>

Download the PSBench repository and cd into scripts

```bash
git clone https://github.com/BioinfoMachineLearning/PSBench.git
cd PSBench
cd scripts
```

#### Openstructure Installation (Need to run only once)
```bash
docker pull registry.scicore.unibas.ch/schwede/openstructure:latest
```

Check the docker installation with 
```bash
# print the installed latest version of openstructure 
docker run -it registry.scicore.unibas.ch/schwede/openstructure:latest --version
``` -->

#### Quality Scores Generation

#### Run the generate_quality_scores.sh pipeline

#### Required Arguments:

| Argument         | Description                                                                                      |
|------------------|--------------------------------------------------------------------------------------------------|
| `--fasta_dir`     | Path to the directory containing FASTA files (named as `<target>.fasta`)                        |
| `--predicted_dir` | Path to the base directory containing predicted models (subdirectory per target)                |
| `--native_dir`    | Path to the directory containing native PDB files (named as `<target>.pdb`)                     |
| `--outdir`        | Path to the base output directory for results                                                   |
| `--usalign`       | Path to the USalign binary (e.g., `tools/USalign`)                                              |
| `--clustalw`      | Path to the ClustalW binary (e.g., `tools/clustalw1.83/clustalw`)                               |
| `--targets`       | Space-separated list of target names to process (e.g., `H1204 H1213`)                           |

For each target (e.g. `H1204`), ensure the following:

- FASTA file: `/path/to/PSBench/Fasta/H1204.fasta`
- Predicted models: `/path/to/PSBench/predicted_models/H1204/*.pdb`
- Native PDB: `/path/to/PSBench/native_models/H1204.pdb`

#### Example:

```bash
cd scripts/
sh generate_quality_scores.sh \
  --fasta_dir /path/to/PSBench/Fasta/ \
  --predicted_dir /path/to/PSBench/predicted_models/ \
  --native_dir /path/to/PSBench/native_models/ \
  --outdir /path/to/PSBench/output/ \
  --usalign /path/to/PSBench/scripts/tools/USalign \
  --clustalw /path/to/PSBench/scripts/tools/clustalw1.83/clustalw \
  --targets H1204 H1213

```
#### Output:
Output folder will have subdirectories for each target (eg. /path/to/PSBench/output/ will have H1204/ H1213/). Each target subdirectory will have the following:

- filtered_pdbs/ : directory where filtered predicted and native structures are saved
- H1204_quality_scores.csv : CSV containing the quuality scores for each model of the target
- results/ : directory where outputs of OpenStructure and USalign runs are saved
- temp/ : temporary directory for pdb filtration process



#### Optional : Generate AlphaFold features when available

<details>

#### Run the generate_af_features.sh pipeline

##### Required Arguments:
| Argument       | Description                                                                                      |
|----------------|--------------------------------------------------------------------------------------------------|
| `--fasta_dir`  | Directory containing FASTA files for each target (e.g., `/path/to/fasta`)                        |
| `--pdb_dir`    | Directory containing predicted PDB model subfolders (e.g., `/path/to/pdbs`)                      |
| `--pkl_dir`    | Directory containing AlphaFold pickle (.pkl) subfolders (e.g., `/path/to/pkls`)                 |
| `--outdir` | Directory where output CSV files will be saved (e.g., `/path/to/outdir`)                    |
| `--targets`    | List of target names (e.g., `H1204 H1213`)                                                       |

For each target (e.g. `H1204`), ensure the following:

- FASTA file: `/path/to/PSBench/Fasta/H1204.fasta`
- Predicted models: `/path/to/PSBench/predicted_models/H1204/*.pdb`
- Pickle files: `/path/to/PSBench/predicted_models_pickles/H1204/*.pkl`

#### Example:

```bash
cd scripts/
sh generate_af_features.sh \
  --fasta_dir /path/to/PSBench/Fasta/ \
  --predicted_dir /path/to/PSBench/predicted_models/ \
  --pkl_dir /path/to/PSBench/predicted_models_pickles/ \
  --outdir /path/to/PSBench/output/ \
  --targets H1204 H1213
```
#### Output:
Output folder will have target_af_features.csv for each target (eg. H1204_af_features.csv).

</details>

## IV. Baseline EMA methods for comparison with a new EMA method

Here are several established methods for Estimating Model Accuracy (EMA), with links to their source code:

- **GATE** [[Liu et al., 2025]](https://github.com/BioinfoMachineLearning/gate):  
  A multi-model EMA approach leveraging graph transformers on pairwise similarity graphs. Combines single-model and multi-model features for TM-score prediction.  
  ğŸ”— GitHub: [https://github.com/BioinfoMachineLearning/gate](https://github.com/BioinfoMachineLearning/gate)  
  - **GATE-AFM**: An enhanced version of GATE that incorporates AlphaFold-Multimer features as node features.

- **DProQA** [[Chen et al., 2023]](https://github.com/jianlin-cheng/DProQA):  
  A single-model EMA method using a Gated Graph Transformer. Targets interface quality prediction (e.g., DockQ scores) using KNN-based structural graphs.  
  ğŸ”— GitHub: [https://github.com/jianlin-cheng/DProQA](https://github.com/jianlin-cheng/DProQA)

- **VoroMQA-dark, VoroIF-GNN-score, VoroIF-GNN-pCAD-score** [[OlechnoviÄ et al., 2023]](https://github.com/kliment-olechnovic/ftdmp):  
  Interface-focused EMA methods using Voronoi-based atomic contact areas and GNNs.  
  ğŸ”— GitHub: [https://github.com/kliment-olechnovic/ftdmp](https://github.com/kliment-olechnovic/ftdmp)

- **GCPNet-EMA** [[Morehead et al., 2024]](https://github.com/BioinfoMachineLearning/GCPNet-EMA):  
  A 3D graph neural network predicting lDDT and global accuracy from atomic point clouds. Adaptable to protein complex structures.  
  ğŸ”— GitHub: [https://github.com/BioinfoMachineLearning/GCPNet-EMA](https://github.com/BioinfoMachineLearning/GCPNet-EMA)

- **PSS (Pairwise Similarity Score)** [[Roy et al., 2023]](https://github.com/BioinfoMachineLearning/MULTICOM_qa):  
  A multi-model consensus method using average pairwise TM-scores (via MMalign).  
  ğŸ”— GitHub: [MULTICOM_qa](https://github.com/BioinfoMachineLearning/MULTICOM_qa)  
  ğŸ”— Simplified: [mmalign_pairwise.py](https://github.com/BioinfoMachineLearning/gate/blob/main/gate/feature/mmalign_pairwise.py)


## Reference
